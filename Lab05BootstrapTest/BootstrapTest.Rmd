---
title: "BootstrapTest"
author: "Win-Vector LLC"
date: "March 16, 2016"
output: html_document
---

## Example task: compare two models and decide which (or if) one is better

Load some example scored data.  See https://github.com/WinVector/zmPDSwR/tree/master/KDD2009 for details.

```{r load}
# Load two models scored on test data from KDD2009 example
# path <- 'https://raw.githubusercontent.com/WinVector/zmPDSwR/master/KDD2009/treatedTestP.tsv'
path <- 'treatedTestP.tsv'
testD <- read.table(path,header=TRUE,sep="\t",
                    stringsAsFactors = FALSE,strip.white = TRUE)
print(head(testD))
```

Plot the ROC/AUC for the gbm model of churn.

```{r gbmAUC}
# devtools::install_github("WinVector/WVPlots")
source('../WVPlots/sourceAllWVPlots.R')
source('functions.R')

ROCPlot(testD,'gbmPred','churn','gbm model')
gbmAUC = as.numeric(pROC::auc(testD$churn,testD$gbmPred))
print(gbmAUC)
```

Plot the ROC/AUC for the glm model of churn.

```{r glmAUC}
ROCPlot(testD,'glmPred','churn','glm model')
glmAUC = as.numeric(pROC::auc(testD$churn,testD$glmPred))
print(glmAUC)
```


GBM has a higher AUC, so appears to perform better.

## Question: are these two AUCs actually signficantly different?

To address this we bootstrap sample evaluation sets of the same size as our
evaluation set and look at the distribution of reported scores. Note that this procedure 
does not involve refitting the models (or even reapplying them), so 
it is not too computationally demanding.

See http://www.statmethods.net/advstats/bootstrapping.html and 
https://cran.r-project.org/doc/Rnews/Rnews_2002-3.pdf for details.

```{r bootstrap}
library('boot')
library('ggplot2')

set.seed(24627)

bs <- function(data, indices, scoreColumn) {
  d <- data[indices,]
  return(pROC::auc(d$churn,d[[scoreColumn]]))
}

resultsGLM <- boot(data=testD, statistic=bs, 
  	R=100,
  	parallel='snow', ncpus=parallel::detectCores(),
  	scoreColumn = 'glmPred')

bci = boot.ci(resultsGLM,type='basic')
print(bci)
ciGLM = bci$basic[, 4:5]

resultsGBM <- boot(data=testD, statistic=bs, 
  	R=100,
  	parallel='snow', ncpus=parallel::detectCores(),
  	scoreColumn = 'gbmPred')

bci = boot.ci(resultsGBM,type='basic')
print(bci)
ciGBM = bci$basic[,4:5]

# auc
pd <- rbind(data.frame(AUC=resultsGLM$t[,1],
                       model='glm',
                       stringsAsFactors = FALSE),
            data.frame(AUC=resultsGBM$t[,1],
                       model='gbm',
                       stringsAsFactors = FALSE))
# observations
mk <- data.frame(AUC=c(glmAUC,gbmAUC),
                 model=c('glm','gbm'),
                 stringsAsFactors = FALSE)

# confidence intervals
ci = as.data.frame(rbind(ciGLM, ciGBM)) 
colnames(ci) = c("lower", "upper")
ci$model= c('glm', 'gbm')

# merge the observed AUC and the confidence intervals
ci = merge(ci, mk)
print(ci)

# plot the observed aucs, aucs of bootstrapped samples, and 95% confidence interval
scattercrossbar(pd, ci, "Observed AUCs, bootstrapped observations, and 95% confidence intervals")

ggplot(pd, aes(x=AUC, color=model)) + geom_density(adjust=0.5) + 
  geom_vline(data=ci, aes(xintercept=AUC, color=model)) +
  geom_text(data=ci, aes(label=paste("observed auc:", model), x=AUC, y=1), hjust="outward", size=5, show.legend=FALSE) + 
  ggtitle("Distribution of bootstrapped samples") + scale_color_brewer(palette="Dark2")
                                                                              

```

## Is glm doing worse than gbm?

Under the assumption that the distribution of glm performance is identical 
to the distribution of gbm performance (gbm performance is the reference
distribution), how unlikely is our observed value glmAUC? Note that this is a one sided test.

```{r specificprobabilityquestion}

plot_densities(pd[pd$model=="gbm",], "AUC", glmAUC, "Estimated probability of observing GBM auc less than observed GLM auc")

# get the probability directly

# ecdf returns a FUNCTION that computes the empirical cdf 
distGBM = ecdf(resultsGBM$t[,1])
print(distGBM(glmAUC))
 
```

## Is gbm doing better than glm?

Conversely, under the assumption that the distribution of glm performance is identical 
to the distribution of gbm performance and glm performance is the reference
distribution, how unlikely is our observed value gbmAUC? This is only a one-sided test.

```{r specificprobabilityquestion2}

plot_densities(pd[pd$model=="glm",], "AUC", gbmAUC, "Estimated probability of observing GLM auc greater than observed GBM auc", tail="right")

# get the probability directly
distGLM = ecdf(resultsGLM$t[,1])
print(1 - distGLM(gbmAUC))  # the right tail
```


## Which model would I pick?

If I were only worried about overall classification or sorting performance, I would pick gbm, even if the difference between the two models doesn't appear to be significant. However, glm has a number of advantages beyond questions of predictive performance: 

* It is computationally more efficient (runs faster)
* Logistic regression is a lower variance fitting procedure than gradient boosting, which means, among other things, it is less sensitive to slight variations in the training set distribution. 
* Logistic regression models are more interpretable

If any of these considerations matter to my application, then I might choose glm instead of gbm, under the reasoning that I gain a number of advantages without significantly sacrificing performance.




